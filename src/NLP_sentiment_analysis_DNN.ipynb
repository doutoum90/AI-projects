{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, LSTM, Dropout, Dense, Flatten, Embedding, Conv1D, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_data = pd.read_csv('../data/Tweets.csv')\n",
    "airline_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_data.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = airline_data['text']\n",
    "y = pd.get_dummies(airline_data.airline_sentiment, prefix='sent').values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    document = remove_tags(doc)\n",
    "    \n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "    \n",
    "    document = re.sub(r'\\s+[^a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    return document\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(document):\n",
    "    return TAG_RE.sub('', document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sentences = []\n",
    "reviews = list()\n",
    "\n",
    "for rev in X:\n",
    "    X_sentences.append(clean_text(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sentences, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dict = dict()\n",
    "glove_embeddings = open('../data/glove.6B.100d.txt', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embeddings in glove_embeddings:\n",
    "    embeddings_tokens = embeddings.split()\n",
    "    emb_word = embeddings_tokens[0]\n",
    "    emb_vector = np.asarray(embeddings_tokens[1:], dtype='float32')\n",
    "    embedded_dict[emb_word] = emb_vector\n",
    "glove_embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12085, 100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd_mat = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embedded_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedd_mat[index] = embedding_vector\n",
    "embedd_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enbedded_model():\n",
    "    embedding_inputs = Input(shape=(maxlen))\n",
    "    embedding_layer = Embedding(vocabulary_size, 100, weights=[embedd_mat], trainable=False)(embedding_inputs)\n",
    "    flatten_layer = Flatten()(embedding_layer)\n",
    "    \n",
    "    dense1 = Dense(512, activation='relu')(flatten_layer)\n",
    "    do1 = Dropout(0.3)(dense1)\n",
    "\n",
    "    dense2 = Dense(512, activation='relu')(do1)\n",
    "    do2 = Dropout(0.3)(dense2)\n",
    "    \n",
    "    dense3 = Dense(512, activation='relu')(do2)\n",
    "    do3 = Dropout(0.3)(dense3)\n",
    "    \n",
    "    output_layer = Dense(y_train.shape[1], activation='softmax')(do3)\n",
    "    return Model(inputs=embedding_inputs, outputs=output_layer)\n",
    "\n",
    "model = enbedded_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 100)          1208500   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               5120512   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 6,855,863\n",
      "Trainable params: 5,647,363\n",
      "Non-trainable params: 1,208,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.8478 - acc: 0.6369 - val_loss: 0.7174 - val_acc: 0.7162\n",
      "Epoch 2/100\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 0.6519 - acc: 0.7395 - val_loss: 0.6971 - val_acc: 0.7264\n",
      "Epoch 3/100\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 0.5525 - acc: 0.7794 - val_loss: 0.6927 - val_acc: 0.7230\n",
      "Epoch 4/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.4789 - acc: 0.8097 - val_loss: 0.7047 - val_acc: 0.7264\n",
      "Epoch 5/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.3704 - acc: 0.8596 - val_loss: 0.7476 - val_acc: 0.7243\n",
      "Epoch 6/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.3067 - acc: 0.8872 - val_loss: 0.9026 - val_acc: 0.7230\n",
      "Epoch 7/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.2518 - acc: 0.9042 - val_loss: 0.9615 - val_acc: 0.7029\n",
      "Epoch 8/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.2063 - acc: 0.9241 - val_loss: 0.9123 - val_acc: 0.7098\n",
      "Epoch 9/100\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.1683 - acc: 0.9386 - val_loss: 1.1112 - val_acc: 0.7111\n",
      "Epoch 10/100\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.1556 - acc: 0.9421 - val_loss: 1.1559 - val_acc: 0.7098\n",
      "Epoch 11/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.1335 - acc: 0.9552 - val_loss: 1.3393 - val_acc: 0.6948\n",
      "Epoch 12/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.1230 - acc: 0.9583 - val_loss: 1.2367 - val_acc: 0.7012\n",
      "Epoch 13/100\n",
      "147/147 [==============================] - 3s 21ms/step - loss: 0.1216 - acc: 0.9586 - val_loss: 1.4991 - val_acc: 0.7106\n",
      "Epoch 14/100\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.1016 - acc: 0.9658 - val_loss: 1.3954 - val_acc: 0.7123\n",
      "Epoch 15/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0942 - acc: 0.9683 - val_loss: 1.3453 - val_acc: 0.6978\n",
      "Epoch 16/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0866 - acc: 0.9688 - val_loss: 1.5156 - val_acc: 0.7140\n",
      "Epoch 17/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0865 - acc: 0.9697 - val_loss: 1.4914 - val_acc: 0.6978\n",
      "Epoch 18/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0945 - acc: 0.9693 - val_loss: 1.5634 - val_acc: 0.7153\n",
      "Epoch 19/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0805 - acc: 0.9706 - val_loss: 1.5146 - val_acc: 0.7175\n",
      "Epoch 20/100\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.0723 - acc: 0.9743 - val_loss: 1.5153 - val_acc: 0.7106\n",
      "Epoch 21/100\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.0659 - acc: 0.9785 - val_loss: 1.5893 - val_acc: 0.7123\n",
      "Epoch 22/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0605 - acc: 0.9809 - val_loss: 1.7015 - val_acc: 0.7093\n",
      "Epoch 23/100\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.0744 - acc: 0.9733 - val_loss: 1.6783 - val_acc: 0.7183\n",
      "Epoch 24/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0578 - acc: 0.9780 - val_loss: 1.5489 - val_acc: 0.6965\n",
      "Epoch 25/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0628 - acc: 0.9793 - val_loss: 1.7142 - val_acc: 0.7106\n",
      "Epoch 26/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0773 - acc: 0.9767 - val_loss: 1.6425 - val_acc: 0.6880\n",
      "Epoch 27/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0611 - acc: 0.9794 - val_loss: 1.5363 - val_acc: 0.7204\n",
      "Epoch 28/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0618 - acc: 0.9795 - val_loss: 1.5233 - val_acc: 0.7243\n",
      "Epoch 29/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0535 - acc: 0.9826 - val_loss: 1.4681 - val_acc: 0.7059\n",
      "Epoch 30/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0492 - acc: 0.9843 - val_loss: 1.8610 - val_acc: 0.7081\n",
      "Epoch 31/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0518 - acc: 0.9827 - val_loss: 1.8044 - val_acc: 0.7093\n",
      "Epoch 32/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0581 - acc: 0.9794 - val_loss: 1.6414 - val_acc: 0.7055\n",
      "Epoch 33/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0661 - acc: 0.9790 - val_loss: 1.7079 - val_acc: 0.7234\n",
      "Epoch 34/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0551 - acc: 0.9816 - val_loss: 1.7650 - val_acc: 0.7093\n",
      "Epoch 35/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0489 - acc: 0.9846 - val_loss: 1.5422 - val_acc: 0.7166\n",
      "Epoch 36/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0479 - acc: 0.9840 - val_loss: 1.6294 - val_acc: 0.7085\n",
      "Epoch 37/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0492 - acc: 0.9837 - val_loss: 1.7200 - val_acc: 0.7128\n",
      "Epoch 38/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0382 - acc: 0.9875 - val_loss: 1.7836 - val_acc: 0.7234\n",
      "Epoch 39/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0449 - acc: 0.9861 - val_loss: 1.7572 - val_acc: 0.7166\n",
      "Epoch 40/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0384 - acc: 0.9889 - val_loss: 1.6797 - val_acc: 0.7034\n",
      "Epoch 41/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0512 - acc: 0.9844 - val_loss: 1.6417 - val_acc: 0.7085\n",
      "Epoch 42/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0416 - acc: 0.9854 - val_loss: 1.8985 - val_acc: 0.7345\n",
      "Epoch 43/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0378 - acc: 0.9863 - val_loss: 2.0156 - val_acc: 0.7132\n",
      "Epoch 44/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0705 - acc: 0.9784 - val_loss: 1.8582 - val_acc: 0.7047\n",
      "Epoch 45/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0385 - acc: 0.9911 - val_loss: 1.8555 - val_acc: 0.6884\n",
      "Epoch 46/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0513 - acc: 0.9819 - val_loss: 1.8029 - val_acc: 0.6983\n",
      "Epoch 47/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0399 - acc: 0.9860 - val_loss: 1.9872 - val_acc: 0.7068\n",
      "Epoch 48/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0492 - acc: 0.9861 - val_loss: 1.7525 - val_acc: 0.7072\n",
      "Epoch 49/100\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.0425 - acc: 0.9859 - val_loss: 1.8348 - val_acc: 0.7064\n",
      "Epoch 50/100\n",
      "147/147 [==============================] - 3s 21ms/step - loss: 0.0395 - acc: 0.9869 - val_loss: 2.0800 - val_acc: 0.7162\n",
      "Epoch 51/100\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.0467 - acc: 0.9841 - val_loss: 1.8794 - val_acc: 0.7200\n",
      "Epoch 52/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0401 - acc: 0.9868 - val_loss: 1.8196 - val_acc: 0.7166\n",
      "Epoch 53/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0476 - acc: 0.9854 - val_loss: 2.2156 - val_acc: 0.7179\n",
      "Epoch 54/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0365 - acc: 0.9863 - val_loss: 1.9385 - val_acc: 0.7089\n",
      "Epoch 55/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0372 - acc: 0.9866 - val_loss: 1.7422 - val_acc: 0.7076\n",
      "Epoch 56/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0326 - acc: 0.9887 - val_loss: 1.7433 - val_acc: 0.6974\n",
      "Epoch 57/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0333 - acc: 0.9894 - val_loss: 2.1197 - val_acc: 0.7076\n",
      "Epoch 58/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0400 - acc: 0.9866 - val_loss: 2.0337 - val_acc: 0.7157\n",
      "Epoch 59/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0398 - acc: 0.9847 - val_loss: 1.8598 - val_acc: 0.7089\n",
      "Epoch 60/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0441 - acc: 0.9859 - val_loss: 2.1210 - val_acc: 0.7012\n",
      "Epoch 61/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0447 - acc: 0.9848 - val_loss: 2.1210 - val_acc: 0.7204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0371 - acc: 0.9888 - val_loss: 1.7779 - val_acc: 0.7175\n",
      "Epoch 63/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0337 - acc: 0.9905 - val_loss: 1.8478 - val_acc: 0.7187\n",
      "Epoch 64/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0498 - acc: 0.9837 - val_loss: 1.8827 - val_acc: 0.7200\n",
      "Epoch 65/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0339 - acc: 0.9890 - val_loss: 1.9697 - val_acc: 0.7017\n",
      "Epoch 66/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0321 - acc: 0.9883 - val_loss: 2.0409 - val_acc: 0.7102\n",
      "Epoch 67/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0412 - acc: 0.9855 - val_loss: 2.3099 - val_acc: 0.7149\n",
      "Epoch 68/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0521 - acc: 0.9838 - val_loss: 1.7239 - val_acc: 0.7029\n",
      "Epoch 69/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0388 - acc: 0.9872 - val_loss: 1.8997 - val_acc: 0.6970\n",
      "Epoch 70/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0323 - acc: 0.9887 - val_loss: 2.3162 - val_acc: 0.7162\n",
      "Epoch 71/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0335 - acc: 0.9899 - val_loss: 1.9122 - val_acc: 0.7051\n",
      "Epoch 72/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0434 - acc: 0.9882 - val_loss: 2.1174 - val_acc: 0.7102\n",
      "Epoch 73/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0430 - acc: 0.9864 - val_loss: 1.9779 - val_acc: 0.7149\n",
      "Epoch 74/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0279 - acc: 0.9903 - val_loss: 2.1044 - val_acc: 0.7111\n",
      "Epoch 75/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0350 - acc: 0.9881 - val_loss: 2.1058 - val_acc: 0.7128\n",
      "Epoch 76/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0266 - acc: 0.9923 - val_loss: 1.9055 - val_acc: 0.7132\n",
      "Epoch 77/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0423 - acc: 0.9858 - val_loss: 1.8277 - val_acc: 0.7089\n",
      "Epoch 78/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0385 - acc: 0.9867 - val_loss: 2.1603 - val_acc: 0.7153\n",
      "Epoch 79/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0300 - acc: 0.9909 - val_loss: 1.9512 - val_acc: 0.7098\n",
      "Epoch 80/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0231 - acc: 0.9931 - val_loss: 1.7325 - val_acc: 0.7047\n",
      "Epoch 81/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0346 - acc: 0.9894 - val_loss: 1.6352 - val_acc: 0.7132\n",
      "Epoch 82/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0331 - acc: 0.9886 - val_loss: 2.1340 - val_acc: 0.7038\n",
      "Epoch 83/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0322 - acc: 0.9877 - val_loss: 1.8715 - val_acc: 0.7128\n",
      "Epoch 84/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0394 - acc: 0.9880 - val_loss: 1.8819 - val_acc: 0.7145\n",
      "Epoch 85/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0240 - acc: 0.9932 - val_loss: 2.4241 - val_acc: 0.7106\n",
      "Epoch 86/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0419 - acc: 0.9868 - val_loss: 1.8132 - val_acc: 0.6833\n",
      "Epoch 87/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0425 - acc: 0.9857 - val_loss: 2.0159 - val_acc: 0.7004\n",
      "Epoch 88/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0257 - acc: 0.9913 - val_loss: 2.0234 - val_acc: 0.7157\n",
      "Epoch 89/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0363 - acc: 0.9888 - val_loss: 2.1124 - val_acc: 0.7145\n",
      "Epoch 90/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0454 - acc: 0.9914 - val_loss: 2.2604 - val_acc: 0.7200\n",
      "Epoch 91/100\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.0299 - acc: 0.9897 - val_loss: 2.4708 - val_acc: 0.6914\n",
      "Epoch 92/100\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.0326 - acc: 0.9907 - val_loss: 2.0432 - val_acc: 0.7029\n",
      "Epoch 93/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0290 - acc: 0.9893 - val_loss: 2.2324 - val_acc: 0.7076\n",
      "Epoch 94/100\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.0330 - acc: 0.9895 - val_loss: 1.9718 - val_acc: 0.6978\n",
      "Epoch 95/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0434 - acc: 0.9857 - val_loss: 2.1680 - val_acc: 0.7038\n",
      "Epoch 96/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0371 - acc: 0.9875 - val_loss: 2.3150 - val_acc: 0.7098\n",
      "Epoch 97/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0333 - acc: 0.9895 - val_loss: 2.1513 - val_acc: 0.7179\n",
      "Epoch 98/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0357 - acc: 0.9891 - val_loss: 2.3863 - val_acc: 0.7162\n",
      "Epoch 99/100\n",
      "147/147 [==============================] - 2s 17ms/step - loss: 0.0326 - acc: 0.9926 - val_loss: 2.3367 - val_acc: 0.7106\n",
      "Epoch 100/100\n",
      "147/147 [==============================] - 2s 16ms/step - loss: 0.0321 - acc: 0.9898 - val_loss: 2.3163 - val_acc: 0.7076\n",
      "1.9165146350860596\n",
      "0.7356557250022888\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=100, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(score[0])\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
